{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from typing_extensions import Self\n",
    "\n",
    "from utils import find_multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_configs = {\n",
    "    \"7B\": dict(n_layer=32, n_head=32, n_embd=4096),\n",
    "    \"13B\": dict(n_layer=40, n_head=40, n_embd=5120),\n",
    "    \"30B\": dict(n_layer=60, n_head=52, n_embd=6656),\n",
    "    \"65B\": dict(n_layer=80, n_head=64, n_embd=8192),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the differnet variants of the LLaMa models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LLaMAConfig:\n",
    "    block_size: int = 2048\n",
    "    vocab_size: int = 32000\n",
    "    padded_vocab_size: Optional[int] = None\n",
    "    n_layer: int = 32\n",
    "    n_head: int = 32\n",
    "    n_embd: int = 4096\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.padded_vocab_size is None:\n",
    "            self.padded_vocab_size = find_multiple(self.vocab_size, 64)\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, name: str) -> Self:\n",
    "        return cls(**llama_configs[name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LLaMAConfig` class is used to store class varibales.<br>\n",
    "Lets understand each of the class variable:<br>\n",
    "\n",
    "- `block_size` : Represents the maximum sequence length the language model can process. \n",
    "- `vocab_size` : Represents the size of vocabular the large language model was trained on. \n",
    "- `n_layer` : Represents total number of transformer block. \n",
    "- `n_head` : Represents total number of head in each transformer block.\n",
    "- `n_embd` : Represents size of embedding. \n",
    "\n",
    "Accoriding to [this](https://twitter.com/karpathy/status/1621578354024677377/) tweet of **Andrej Karpathy**, it is important to find the nearest multiple of 64 for your vocab. The tweet explains : <br>\n",
    "\n",
    "*The most dramatic optimization to nanoGPT so far (~25% speedup) is to simply increase vocab size from 50257 to 50304 (nearest multiple of 64). This calculates added useless dimensions but goes down a different kernel path with much higher occupancy. Careful with your Powers of 2.*\n",
    "\n",
    "You can also read more about it [HERE](https://pytorch.org/blog/accelerating-large-language-models/).\n",
    "\n",
    "\n",
    "\n",
    "```pyton\n",
    "\n",
    "def __post_init__(self):\n",
    "    if self.padded_vocab_size is None:\n",
    "        self.padded_vocab_size = find_multiple(self.vocab_size, 64)\n",
    "```\n",
    "So, this code initializes the padded_vocab_size attribute of an object to a multiple of 64 based on the object's vocab_size, but only if padded_vocab_size is not already set.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMA(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        assert config.padded_vocab_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.padded_vocab_size, bias=False)\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.padded_vocab_size, config.n_embd),\n",
    "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                ln_f=RMSNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n",
    "\n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        _, t = idx.size()\n",
    "        assert (\n",
    "            t <= self.config.block_size\n",
    "        ), f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "\n",
    "        # forward the LLaMA model itself\n",
    "        x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)  # (b, t, vocab_size)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, name: str) -> Self:\n",
    "        return cls(LLaMAConfig.from_name(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLaMA model provided is a PyTorch-based implementation. Below is an elaboration on the various components of the code:\n",
    "\n",
    "1. **Initialization**:\n",
    "```python\n",
    "class LLaMA(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        assert config.padded_vocab_size is not None\n",
    "        self.config = config\n",
    "```\n",
    "Here, the model takes a configuration object, `LLaMAConfig`, during initialization. An assertion checks that the `padded_vocab_size` attribute is not `None`.\n",
    "\n",
    "2. **Model Architecture**:\n",
    "```python\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.padded_vocab_size, bias=False)\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.padded_vocab_size, config.n_embd),\n",
    "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                ln_f=RMSNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "```\n",
    "- The `lm_head` is a final linear layer of the large language model to generate the final prediction. It  maps from embeddings to the vocabulary size, which is used for predicting the next word/token. So why are we doing this? This is because we want to represent the probability distribution over the vocabulary to make the prediction. \n",
    "\n",
    "- `transformer` is a dictionary of modules, which includes:\n",
    "  - `wte`: Word Token Embedding, an embedding layer for the vocabulary. Given tokens, it will generate embeddings of size `config.n_embd=4096.`\n",
    "  - `h`: A list of blocks, with each block being a segment of the transformer architecture. The number of blocks is defined by `config.n_layer`.\n",
    "  - `ln_f`: A final layer normalization, here using RMSNorm.\n",
    "\n",
    "3. **Weight Initialization**:\n",
    "```python\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        ...\n",
    "```\n",
    "This method initializes the weights of linear and embedding layers based on the model configuration.\n",
    "\n",
    "4. **Forward Pass**:\n",
    "```python\n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        ...\n",
    "```\n",
    "The forward method defines how input data is processed through the model to produce an output. It processes the input tensor, passes it through the transformer blocks, and eventually through the language model head to produce the logits.<br>\n",
    "\n",
    "Here, `idx` is shape of `B,T`. We havent converted the tokens into embedding.<br>\n",
    "`_, t = idx.size()`  : Get the sequence length. \n",
    "\n",
    "```python\n",
    "assert (\n",
    "    t <= self.config.block_size\n",
    "), f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "```\n",
    "This will check whether the input sequence is greater than the max sequence length i.e. `self.config.block_size`. \n",
    "\n",
    "\n",
    "``x = self.transformer.wte(idx) `` This will convert the input of sahpe `B,T` to `B,T,n_embd`\n",
    "```python\n",
    "for block in self.transformer.h:\n",
    "    x = block(x)\n",
    "x = self.transformer.ln_f(x)\n",
    "```\n",
    "This passes the embedding through out n transformer blocks. I think this the is the most interesting part in our entire code. \n",
    "We will dive deeper into it next. \n",
    "\n",
    "As discussed above `logits = self.lm_head(x)  # (b, t, vocab_size)` maps from embeddings to the vocabulary size, which is used for predicting the next word/token.\n",
    "\n",
    "\n",
    "\n",
    "5. **Load Model by Name**:\n",
    "```python\n",
    "    @classmethod\n",
    "    def from_name(cls, name: str) -> Self:\n",
    "        return cls(LLaMAConfig.from_name(name))\n",
    "```\n",
    "This class method allows for creating a LLaMA model instance directly using a name, assuming the `LLaMAConfig.from_name(name)` can produce the necessary configuration from the provided name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.rms_1 = RMSNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.rms_2 = RMSNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.rms_1(x))\n",
    "        x = x + self.mlp(self.rms_2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformer block typically consists of self-attention mechanisms followed by feed-forward neural networks. The LLaMA model has infused some variations, including the use of RMSNorm for normalization. \n",
    "**Forward Pass:**\n",
    "- The input tensor x is first normalized using the first RMSNorm instance.\n",
    "- Post normalization, it's fed into the CausalSelfAttention. The result is combined with the original tensor via a residual connection, a vital feature in deep networks for maintaining gradient flow.\n",
    "- The tensor then undergoes the second RMSNorm normalization.\n",
    "- The normalized output is processed by the MLP. As before, the resultant is added back to the tensor using a residual connection.\n",
    "- The processed tensor, rich with information, is then returned.\n",
    "\n",
    "\n",
    "The Block class crystallizes a singular transformer layer's operations within LLaMA. With the integral role of RMSNorm already understood, it becomes evident how this block combines normalization, attention, and feed-forward operations to refine the data representation at each layer. When stacked, these blocks work in concert, building upon one another to offer the powerful capabilities of the LLaMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.block_size = config.block_size\n",
    "        self.rope_cache: Optional[torch.Tensor] = None\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "\n",
    "        head_size = C // self.n_head\n",
    "        k = k.view(B, T, self.n_head, head_size)\n",
    "        q = q.view(B, T, self.n_head, head_size)\n",
    "        v = v.view(B, T, self.n_head, head_size)\n",
    "        \n",
    "        if self.rope_cache is None:\n",
    "            # cache for future forward calls\n",
    "            self.rope_cache = build_rope_cache(\n",
    "                seq_len=self.block_size,\n",
    "                n_elem=self.n_embd // self.n_head, \n",
    "                dtype=x.dtype,\n",
    "                device=x.device,\n",
    "            )\n",
    "\n",
    "        \n",
    "        q = apply_rope(q, self.rope_cache)\n",
    "        k = apply_rope(k, self.rope_cache)\n",
    "\n",
    "        k = k.transpose(1, 2)  # (B, nh, T, hs)\n",
    "        q = q.transpose(1, 2)  # (B, nh, T, hs)\n",
    "        v = v.transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        #  att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        #  att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        #  att = F.softmax(att, dim=-1)\n",
    "        #  y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "\n",
    "        # efficient attention using Flash Attention CUDA kernels\n",
    "        y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=True)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the most interesting part of our LLM. Lets dive into each line of code in details. \n",
    "1. **Initialization:**\n",
    "- Here, we first ensure that the embedding size (n_embd) is divisible by the number of attention heads (n_head). This is necessary to equally distribute the embeddings across all heads.\n",
    "\n",
    "- **The Key, Query, Value Projections**:<br>\n",
    "    ```self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)```\n",
    "\n",
    "    This transformation is designed to produce key, query, and value tensors, which are essential for the attention mechanism. Normally, you'd expect three separate linear transformations - one for each of key, query, and value. But here, they're combined into a single transformation for efficiency.\n",
    "\n",
    "\n",
    "    Input: config.n_embd represents the embedding size of each token in the model.\n",
    "    Output: 3 * config.n_embd might look a bit confusing initially, but it makes perfect sense once you understand the purpose. Since we're generating three different tensors (key, query, and value) and each has an embedding size of config.n_embd, the combined size is 3 * config.n_embd.\n",
    "\n",
    "\n",
    "2. **Forward Pass**:\n",
    "\n",
    "- The input tensor's dimensions are extracted, where:\n",
    "  - `B` represents the batch size.\n",
    "  - `T` stands for the sequence length.\n",
    "  - `C` denotes the embedding dimensionality.\n",
    "  \n",
    "- The tensor `x` undergoes the `c_attn` transformation, splitting the result into query, key, and value tensors (`q, k, v`). \n",
    "\n",
    "- These tensors are then reshaped for multi-head attention. Essentially, the embedding dimensionality is divided among the number of attention heads.\n",
    "\n",
    "- If the rope cache hasn't been built (i.e., `self.rope_cache is None`), it's constructed using the `build_rope_cache` function. As we alrady discussed this cache is calculate for single head and later applied across each head, we can see that `n_elem=self.n_embd // self.n_head`, this basically means for each token in the sequence, we split the token into `n_head` and based on dimension of head, we calculate the ROPE cache. This method is preety much similar to the one we have implemented before. We will discuss some changes in this implementation later. This cache is then applied to the `q` and `k` tensors using `apply_rope` which is also preety much similar to our previous approach. \n",
    "\n",
    "- The `q`, `k`, and `v` tensors are transposed to align them for the attention mechanism. Can you tell why are we performig this transformation? \n",
    "After transposing, we have final tensor of sahpe `(B, nh, T, hs)`. Now if we perform the operation `q @ k.t`, as the key is transformed, final tensor will be of shape `T,T`. This `T,T` matrix will gave us information about, given a token, what's the relation with other tokens. I think you got an idea why this tranformation is performed. This is done basically to get the attention matrix. \n",
    "\n",
    "\n",
    "- The main action happens in the causal self-attention mechanism. Normally, one would compute attention scores by multiplying `q` and `k`, apply a mask for causality, then use this to weight the `v` tensor. Here, however, the mechanism uses the efficient `F.scaled_dot_product_attention` method, which leverages FlashAttention for faster attention calculations. FlashAttention is a new algorithm to speed up attention and reduce its memory footprint—without any approximation.\n",
    "You can read more about FlashAttention [Here](https://crfm.stanford.edu/2023/07/17/flash2.html#:~:text=FlashAttention%20is%20an%20algorithm%20that,to%20linear%20in%20sequence%20length.), [Here](https://www.adept.ai/blog/flashier-attention), [Here](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad).\n",
    "\n",
    "- The resultant tensor `y` is reshaped and then undergoes the output projection via the `c_proj` transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        hidden_dim = 4 * config.n_embd\n",
    "        n_hidden = int(2 * hidden_dim / 3)\n",
    "        n_hidden = find_multiple(n_hidden, 256)\n",
    "\n",
    "        self.c_fc1 = nn.Linear(config.n_embd, n_hidden, bias=False)\n",
    "        self.c_fc2 = nn.Linear(config.n_embd, n_hidden, bias=False)\n",
    "        self.c_proj = nn.Linear(n_hidden, config.n_embd, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.silu(self.c_fc1(x)) * self.c_fc2(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RMSNorm(nn.Module):\n",
    "\n",
    "#     def __init__(self, input_dim , eps = 1e-6) -> None:\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.scale = nn.Parameter(torch.ones(input_dim))\n",
    "#         self.eps = eps\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         # RMS of input\n",
    "#         rms = torch.rsqrt(torch.square(x).mean(dim=-1,keepdim=True) + self.eps)\n",
    "#         # rescaling \n",
    "#         x  = x * rms\n",
    "#         return x * self.scale\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\n",
    "\n",
    "    Derived from https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. BSD 3-Clause License:\n",
    "    https://github.com/bzhangGo/rmsnorm/blob/master/LICENSE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size: int, dim: int = -1, eps: float = 1e-5) -> None:\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(size))\n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # NOTE: the original RMSNorm paper implementation is not equivalent\n",
    "        # norm_x = x.norm(2, dim=self.dim, keepdim=True)\n",
    "        # rms_x = norm_x * d_x ** (-1. / 2)\n",
    "        # x_normed = x / (rms_x + self.eps)\n",
    "        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)\n",
    "        x_normed = x * torch.rsqrt(norm_x + self.eps)\n",
    "        return self.scale * x_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rope_cache(seq_len: int, n_elem: int, dtype: torch.dtype, device: torch.device, base: int = 10000) -> torch.Tensor:\n",
    "    \"\"\"Enhanced Transformer with Rotary Position Embedding.\n",
    "\n",
    "    Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n",
    "    transformers/rope/__init__.py. MIT License:\n",
    "    https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n",
    "    \"\"\"\n",
    "    # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
    "    theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, dtype=dtype, device=device) / n_elem))\n",
    "\n",
    "    # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
    "    seq_idx = torch.arange(seq_len, dtype=dtype, device=device)\n",
    "\n",
    "    # Calculate the product of position index and $\\theta_i$\n",
    "    idx_theta = torch.outer(seq_idx, theta).float()\n",
    "\n",
    "    cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n",
    "\n",
    "    # this is to mimic the behaviour of complex32, else we will get different results\n",
    "    if dtype in (torch.float16, torch.bfloat16, torch.int8):\n",
    "        cache = cache.half()\n",
    "    return cache\n",
    "\n",
    "\n",
    "def apply_rope(x: torch.Tensor, rope_cache: torch.Tensor) -> torch.Tensor:\n",
    "    # truncate to support variable sizes\n",
    "    T = x.size(1)\n",
    "    rope_cache = rope_cache[:T]\n",
    "\n",
    "    # cast because the reference does\n",
    "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "    # uta hami lea cos ra sine lai 2 ota use garinthiyo. Like x_rope, neg_half_x calculate gareko.\n",
    "    rope_cache = rope_cache.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n",
    "    x_out2 = torch.stack(\n",
    "        [\n",
    "            xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],\n",
    "            xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],\n",
    "        ],\n",
    "        -1,\n",
    "    )\n",
    "\n",
    "    x_out2 = x_out2.flatten(3)\n",
    "    return x_out2.type_as(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `build_rope_cache` function is almost identical to `build_cache` we implemented. Here the cos and sin values are calulated before hand. Also, `build_rope_cache` has specific handling for certain data types like torch.float16, torch.bfloat16, and torch.int8, where it casts the computed cache to half precision.\n",
    "`build_cache` doesn't handle data types in this manner.\n",
    "\n",
    "The `apply_rope` is also applies RoPE cache to query and key. But there is slight difference on how the transformation is applied. I'll expain what is happeing in this method in details. \n",
    "\n",
    "We have two tensors: `x` and `rope_cache`. \n",
    "\n",
    "Lets assume `x` is a 4D tensor with shape `(1, 4, 2, 4)` and `rope_cache` is a 4D tensor with shape `(4, 2, 2)`.\n",
    "\n",
    "```python \n",
    "\n",
    "x = tensor([[[[ 0,  1,  2,  3],\n",
    "          [ 4,  5,  6,  7]],\n",
    "\n",
    "         [[ 8,  9, 10, 11],\n",
    "          [12, 13, 14, 15]],\n",
    "\n",
    "         [[16, 17, 18, 19],\n",
    "          [20, 21, 22, 23]],\n",
    "\n",
    "         [[24, 25, 26, 27],\n",
    "          [28, 29, 30, 31]]]])\n",
    "```\n",
    "\n",
    "**Step 1 :** \n",
    "\n",
    "```python\n",
    "T = x.size(1)\n",
    "\n",
    "```\n",
    "\n",
    "Here, `T` is simply the size of the second dimension of `x`, which is 4.\n",
    "\n",
    "**Step 2 :** \n",
    "\n",
    "Next, We resize `rope_cache` to match the size `T`:\n",
    "\n",
    "```python\n",
    "rope_cache = rope_cache[:T]\n",
    "\n",
    "```\n",
    "\n",
    "This step is redundant because `rope_cache` already has a size of 4 in its first dimension.\n",
    "\n",
    "**Step 3:** \n",
    "\n",
    "Then, you reshape `x` to make its last dimension into two parts:\n",
    "\n",
    "```python\n",
    "xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "\n",
    "```\n",
    "\n",
    "This breaks down as:\n",
    "\n",
    "1. Convert x into float: `x.float()`\n",
    "2. Reshape it: For our tensor, this converts it from shape `(1, 4, 2, 4)` to `(1, 4, 4, 2)`.\n",
    "\n",
    "Given the **`xshaped`** tensor structure you provided, we can see that its shape is (1, 4, 2, 2, 2). That means you have:\n",
    "\n",
    "- 1 batch (the outermost dimension)\n",
    "- 4 channels\n",
    "- 2x2 spatial dimensions (height x width)\n",
    "- 2 values for each spatial position (the innermost dimension)\n",
    "\n",
    "For instance, before reshaping, the first 2x4 matrix in `x` is:\n",
    "\n",
    "```\n",
    "0,  1,  2,  3\n",
    "4,  5,  6,  7\n",
    "\n",
    "```\n",
    "\n",
    "After reshaping, the first 4x2 matrix in `xshaped` would be:\n",
    "\n",
    "```\n",
    "0,  1\n",
    "2,  3\n",
    "4,  5\n",
    "6,  7\n",
    "\n",
    "```\n",
    "\n",
    "Next, you are reshaping the `rope_cache`:\n",
    "\n",
    "```python\n",
    "rope_cache = rope_cache.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n",
    "\n",
    "```\n",
    "\n",
    "This converts `rope_cache` from shape `(4, 2, 2)` to `(1, 4, 1, 2, 2)`. This reshaping is done to align the dimensions of `rope_cache` with `xshaped` for broadcasting during the subsequent operations.\n",
    "\n",
    "**Step 3:** \n",
    "\n",
    "Then, you perform element-wise multiplication and subtraction/addition between the reshaped `x` and `rope_cache`:\n",
    "\n",
    "```python\n",
    "x_out2 = torch.stack(\n",
    "    [\n",
    "        xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],\n",
    "        xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],\n",
    "    ],\n",
    "    -1,\n",
    ")\n",
    "```\n",
    "\n",
    "This is similar to performing rotation using sine and cosine values from `rope_cache`. The resulting tensor `x_out2` has the same shape as `xshaped`, which is `(1, 4, 4, 2)`. Rotation operation in **`torch.stack`** would work element-wise over the tensors. This means that for each position in **`xshaped`**, it uses the corresponding position in **`rope_cache`** for the rotation calculation.\n",
    "\n",
    "**Breakdown:**\n",
    "\n",
    "Given a 2D rotation matrix:\n",
    "\n",
    "$$\n",
    "R(\\theta) = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "When you multiply this rotation matrix with a 2D vector $([x, y]^T)$, you get:\n",
    "\n",
    "$$\n",
    "R(\\theta) \\cdot \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} x\\cos(\\theta) - y\\sin(\\theta) \\\\ x\\sin(\\theta) + y\\cos(\\theta) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, let's connect this to the operations in the code:\n",
    "\n",
    "- The first component of the output:\n",
    "$x' = x\\cos(\\theta) - y\\sin(\\theta)$ is given by:\n",
    "`xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1]`\n",
    "\n",
    "Where:\n",
    "\n",
    "- `xshaped[..., 0]` corresponds to the x component (or the first value) of our vector.\n",
    "- `xshaped[..., 1]` corresponds to the y component (or the second value) of our vector.\n",
    "- `rope_cache[..., 0]` is the cosine of the rotation angle.\n",
    "- `rope_cache[..., 1]` is the sine of the rotation angle.\n",
    "- The second component of the output:\n",
    "$y' = x\\sin(\\theta) + y\\cos(\\theta)$ is given by:\n",
    "`xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1]`\n",
    "\n",
    "The code is essentially applying this rotation to every pair of values in the tensor `xshaped` using the angles specified in `rope_cache`.\n",
    "\n",
    "The `torch.stack(..., -1)` at the end stacks these computed values along the last dimension. After this operation, for every pair of x and y values in the original `xshaped`, you have their rotated counterparts stacked together in the resulting tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference \n",
    "\n",
    "For inference we will be using pipeline provided by the lit-lama repo. It provides some helpful classes that can potentially speed up the loading and initialization of large models, especially when only parts of the model need to be accessed or when specific tensor initializations are desired. The code also seems to handle some advanced features like quantization and lazy loading of tensors.\n",
    "\n",
    "let's break down these classes:\n",
    "\n",
    "1. **`EmptyInitOnDevice` class**:\n",
    "\n",
    "   This class is a context manager that changes the behavior of tensor initialization to create tensors with uninitialized memory (or \"empty tensors\"). Additionally, it can set specific devices and data types for tensor initialization, and supports specific quantization modes. When this context is active, tensors are initialized without actually assigning them any initial values, making the initialization process faster in some scenarios.\n",
    "   \n",
    "\n",
    "2. **`NotYetLoadedTensor` class**:\n",
    "\n",
    "   Represents a tensor that has not yet been loaded into memory. It is essentially a placeholder that can be transformed into an actual tensor when accessed or used in computations. This class can be especially useful when dealing with large datasets or models, as it allows for lazy loading of data, only loading tensors into memory when they're actually needed.\n",
    "\n",
    "   \n",
    "3. **`LazyLoadingUnpickler` class**:\n",
    "\n",
    "   Custom unpickler for lazy loading. Pickling is the process of converting a Python object into a byte stream, and unpickling is the reverse operation. The idea here is to load tensors and related objects from the pickled format only when they're actually accessed or used.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "\n",
    "from tokenizer import  Tokenizer\n",
    "from utils import EmptyInitOnDevice, lazy_load, llama_model_lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model: torch.nn.Module,\n",
    "    idx: torch.Tensor,\n",
    "    max_new_tokens: int,\n",
    "    max_seq_length: int,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = None,\n",
    "    eos_id: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n",
    "\n",
    "    The implementation of this function is modified from A. Karpathy's nanoGPT.\n",
    "\n",
    "    Args:\n",
    "        model: The model to use.\n",
    "        idx: Tensor of shape (T) with indices of the prompt sequence.\n",
    "        max_new_tokens: The number of new tokens to generate.\n",
    "        max_seq_length: The maximum sequence length allowed.\n",
    "        temperature: Scales the predicted logits by 1 / temperature\n",
    "        top_k: If specified, only sample among the tokens with the k highest probabilities\n",
    "        eos_id: If specified, stop generating any more token once the <eos> token is triggered\n",
    "    \"\"\"\n",
    "    # create an empty tensor of the expected final shape and fill in the current tokens\n",
    "    T = idx.size(0)\n",
    "    T_new = T + max_new_tokens\n",
    "    empty = torch.empty(T_new, dtype=idx.dtype, device=idx.device)\n",
    "    empty[:T] = idx\n",
    "    idx = empty\n",
    "\n",
    "    # generate max_new_tokens tokens\n",
    "    for t in range(T, T_new):\n",
    "        # ignore the not-filled-yet tokens\n",
    "        idx_cond = idx[:t]\n",
    "        # if the sequence context is growing too long we must crop it at max_seq_length\n",
    "        idx_cond = idx_cond if T <= max_seq_length else idx_cond[-max_seq_length:]\n",
    "\n",
    "        # forward\n",
    "        logits = model(idx_cond.view(1, -1))\n",
    "        logits = logits[0, -1] / temperature\n",
    "\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[[-1]]] = -float(\"Inf\")\n",
    "\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # concatenate the new generation\n",
    "        idx[t] = idx_next\n",
    "\n",
    "        # if <eos> token is triggered, return the output (stop generation)\n",
    "        if idx_next == eos_id:\n",
    "            return idx[:t + 1]  # include the EOS token\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    prompt: str = \"Hello, my name is\",\n",
    "    *,\n",
    "    num_samples: int = 1,\n",
    "    max_new_tokens: int = 50,\n",
    "    top_k: int = 200,\n",
    "    temperature: float = 0.8,\n",
    "    checkpoint_path: Optional[Path] = None,\n",
    "    tokenizer_path: Optional[Path] = None,\n",
    "    quantize: Optional[str] = None,\n",
    ") -> None:\n",
    "    \"\"\"Generates text samples based on a pre-trained LLaMA model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "        prompt: The prompt string to use for generating the samples.\n",
    "        num_samples: The number of text samples to generate.\n",
    "        max_new_tokens: The number of generation steps to take.\n",
    "        top_k: The number of top most probable tokens to consider in the sampling process.\n",
    "        temperature: A value controlling the randomness of the sampling process. Higher values result in more random\n",
    "            samples.\n",
    "        checkpoint_path: The checkpoint path to load.\n",
    "        tokenizer_path: The tokenizer path to load.\n",
    "        quantize: Whether to quantize the model and using which method:\n",
    "            ``\"llm.int8\"``: LLM.int8() mode,\n",
    "            ``\"gptq.int4\"``: GPTQ 4-bit mode.\n",
    "    \"\"\"\n",
    "    if not checkpoint_path:\n",
    "        checkpoint_path = Path(f\"./checkpoints/lit-llama/7B/lit-llama.pth\")\n",
    "    if not tokenizer_path:\n",
    "        tokenizer_path = Path(\"./checkpoints/lit-llama/tokenizer.model\")\n",
    "    assert checkpoint_path.is_file(), checkpoint_path\n",
    "    assert tokenizer_path.is_file(), tokenizer_path\n",
    "\n",
    "    fabric = L.Fabric(devices=1)\n",
    "    dtype = torch.bfloat16 if fabric.device.type == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float32\n",
    "\n",
    "    print(\"Loading model ...\", file=sys.stderr)\n",
    "    t0 = time.time()\n",
    "    with lazy_load(checkpoint_path) as checkpoint:\n",
    "        name = llama_model_lookup(checkpoint)\n",
    "\n",
    "        with EmptyInitOnDevice(\n",
    "                device=fabric.device, dtype=dtype, quantization_mode=quantize\n",
    "        ):\n",
    "            model = LLaMA.from_name(name)\n",
    "\n",
    "        model.load_state_dict(checkpoint)\n",
    "    print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n",
    "\n",
    "    model.eval()\n",
    "    model = fabric.setup_module(model)\n",
    "\n",
    "    tokenizer = Tokenizer(tokenizer_path)\n",
    "    encoded_prompt = tokenizer.encode(prompt, bos=True, eos=False, device=fabric.device)\n",
    "\n",
    "    L.seed_everything(1234)\n",
    "    for i in range(num_samples):\n",
    "        t0 = time.perf_counter()\n",
    "        y = generate(\n",
    "            model,\n",
    "            encoded_prompt,\n",
    "            max_new_tokens,\n",
    "            model.config.block_size,  # type: ignore[union-attr,arg-type]\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        t = time.perf_counter() - t0\n",
    "        print('\\n\\n')\n",
    "        print(tokenizer.decode(y))\n",
    "        print('\\n\\n')\n",
    "        print(f\"Time for inference {i + 1}: {t:.02f} sec total, {max_new_tokens / t:.02f} tokens/sec\", file=sys.stderr)\n",
    "    if fabric.device.type == \"cuda\":\n",
    "        print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model ...\n",
      "Time to load model: 17.45 seconds.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Global seed set to 1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Artificial Intelligence is the ability of a computer to imitate intelligent behaviour without being programmed, such as learning in a self-directed way to do a specific task, and then not just repeating the task, but improving itself. This is different from Traditional Artificial Intelligence which is any\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 1: 1.41 sec total, 35.55 tokens/sec\n",
      "Memory used: 13.52 GB\n"
     ]
    }
   ],
   "source": [
    "main(\"Artificial Intelligence is the\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rajan_doc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
